{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Bio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mmminbay\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mbait_research\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcluster_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClusterHelper\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# from fasta_utils import sequences_from_fasta, sequences_to_fasta\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hamming_distance\n",
      "File \u001b[0;32m~/honors_thesis/bait_research/cluster_tools.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SeqIO\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Bio'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\mminbay\\\\Documents\\\\bait_research\\\\code')\n",
    "from cluster_tools import ClusterHelper\n",
    "# from fasta_utils import sequences_from_fasta, sequences_to_fasta\n",
    "# from utils import hamming_distance\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "from pydivsufsort import divsufsort, sa_search\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "SEQUENCES_DIR: str = 'C:\\\\Users\\\\mminbay\\\\Documents\\\\bait_research\\\\test_data'\n",
    "CLUSTERS_DIR: str = 'C:\\\\Users\\\\mminbay\\\\Documents\\\\bait_research\\\\clusters'\n",
    "OUTPUTS_DIR: str = 'C:\\\\Users\\\\mminbay\\\\Documents\\\\bait_research\\\\outputs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_report(report_path: str):\n",
    "    stupid_alignments = {}\n",
    "    last_num = r\"[0-9]+\\.$\"\n",
    "    with open(report_path, 'r') as file:\n",
    "        line = file.readline()\n",
    "        while line:\n",
    "            line = line.strip()\n",
    "            if 'Identifier' in line:\n",
    "                for i in range(2):\n",
    "                    line = file.readline() # skip 2 lines \n",
    "                line = line.strip()\n",
    "                rep = int(re.search(last_num, line).group()[:-1])\n",
    "                for i in range(6):\n",
    "                    line = file.readline() # skip 6\n",
    "                line = line.strip()[1:-1]\n",
    "                items = line.split(', ')\n",
    "                items = [int(x) for x in items]\n",
    "                stupid_alignments[rep] = items\n",
    "            line = file.readline()\n",
    "    return stupid_alignments  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_decide(str1, str2, m):\n",
    "    '''\n",
    "    '''\n",
    "    distance = 0\n",
    "    for i in range(len(str1)):\n",
    "        if str1[i] == 'N': # character N does not match with anything\n",
    "            distance += 1 \n",
    "        elif str1[i] != str2[i]:\n",
    "            distance += 1\n",
    "        if distance > m:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences read:  1\n",
      "Total length:  100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mminbay\\AppData\\Local\\anaconda3\\envs\\bait_research\\lib\\site-packages\\pydivsufsort\\divsufsort.py:75: UserWarning: converting str argument uses more memory\n",
      "  warnings.warn(\"converting str argument uses more memory\")\n"
     ]
    }
   ],
   "source": [
    "# SETUP BLOCK\n",
    "sequence_name = 'megares_100k_sequences.fasta'\n",
    "_, seq = sequences_from_fasta(os.path.join(SEQUENCES_DIR, sequence_name))\n",
    "seqlens = [len(x) for x in seq]\n",
    "seq = ''.join(seq)\n",
    "seq = seq.upper()\n",
    "sa = divsufsort(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_and_extend(\n",
    "    seq: str,\n",
    "    bait: str,\n",
    "    m: int,\n",
    "    sa = None,\n",
    "    seed_len: int = 10,\n",
    "    seqlens = None,\n",
    "):\n",
    "    '''\n",
    "    Applies the seed-and-extend heuristic to search for imperfect matches of the bait\n",
    "    in the sequence. Returns a list of indices of the matches.\n",
    "\n",
    "    Arguments:\n",
    "        seq (str): Sequence to search for the baits in. \n",
    "        bait (str): Bait to search for.\n",
    "        m (int): Allowed Hamming distance for each match.\n",
    "        sa (suffix array): Suffix array of the string. If not provided, will be constructed.\n",
    "        seed_len (int): Seed length to use for the heuristic. Every seed-length substring of the \n",
    "            bait will be used as seeds.\n",
    "        seqlens (list[int]): If `seq` was obtained by concatenating sequences, this list \n",
    "            should have the lengths of each individual sequence before concatenation. This \n",
    "            is because baits should not be able to align around the concatenation points, so\n",
    "            these values will be used to eliminate those baits.\n",
    "    '''\n",
    "    if sa is None:\n",
    "        checkpoint = time.time()\n",
    "        sa = divsufsort(seq)\n",
    "        print('Divsufsort took {} seconds.'.format(time.time() - checkpoint))\n",
    "    if seqlens is None:\n",
    "        seqlens = [len(seq)]\n",
    "\n",
    "    lb = len(bait)\n",
    "    if seed_len > lb:\n",
    "        raise Exception('Specified seed length is larger than the actual bait.')\n",
    "    \n",
    "    final_matches = set()\n",
    "    checked_indices = set()\n",
    "\n",
    "    lookup_time = 0\n",
    "    discard_time = 0\n",
    "    alignment_time = 0\n",
    "    d_picked = 0\n",
    "    d_checked = 0\n",
    "    d_concat = 0\n",
    "    d_negative = 0\n",
    "    \n",
    "    # for i in tqdm(range(lb - seed_len + 1), desc = 'Considering seeds', unit = 'seed'):\n",
    "    for i in range(lb - seed_len + 1):\n",
    "        checkpoint = time.time()\n",
    "        seed = bait[i: i + seed_len]\n",
    "        seed_matches = sa_search(seq, sa, seed)\n",
    "        lookup_time += time.time() - checkpoint\n",
    "        if seed_matches[1] is None:\n",
    "            continue\n",
    "        seed_matches = set(sa[seed_matches[1]: seed_matches[0] + seed_matches[1]])\n",
    "        checkpoint = time.time()\n",
    "        sum_seqlens = 0\n",
    "        for seqlen in seqlens:\n",
    "            discard_matches = set()\n",
    "            sum_seqlens += seqlen\n",
    "            for match in seed_matches:\n",
    "                # find actual start of this alignment and check if it covers a concatenation\n",
    "                actual_start = match - i\n",
    "                if actual_start in final_matches: # if we already checked the coverage\n",
    "                    discard_matches.add(match)     \n",
    "                    d_picked += 1\n",
    "                elif actual_start in checked_indices:\n",
    "                    discard_matches.add(match)\n",
    "                    d_checked += 1\n",
    "                elif actual_start < 0:\n",
    "                    discard_matches.add(match)\n",
    "                    d_negative += 1\n",
    "                elif actual_start < sum_seqlens and actual_start > sum_seqlens - lb:\n",
    "                    discard_matches.add(match)\n",
    "                    d_concat += 1\n",
    "                checked_indices.add(actual_start)\n",
    "            seed_matches = seed_matches.difference(discard_matches) # VERY INEFFICIENT - change\n",
    "        discard_time += time.time() - checkpoint\n",
    "        checkpoint = time.time()\n",
    "        for match in seed_matches:\n",
    "            actual_start = match - i\n",
    "            sub = seq[actual_start: actual_start + lb]\n",
    "            if len(sub) != len(bait):\n",
    "                print(actual_start, 'wtf happened here')\n",
    "            if hamming_decide(bait, sub, m):\n",
    "                final_matches.add(actual_start)\n",
    "        alignment_time += time.time() - checkpoint\n",
    "    # print('Lookup time:', lookup_time)\n",
    "    # print('Discard time:', discard_time)\n",
    "    # print('Alignment time:', alignment_time)\n",
    "    # print('Discarded because picked:', d_picked)\n",
    "    # print('Discarded because checked:', d_checked)\n",
    "    # print('Discarded because negative:', d_negative)\n",
    "    # print('Discarded because concat:', d_concat)\n",
    "    # print('Found alignments:', len(final_matches))\n",
    "    return final_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences read:  347\n",
      "Total length:  41640\n"
     ]
    }
   ],
   "source": [
    "_, baits = sequences_from_fasta(os.path.join(OUTPUTS_DIR, 'megares_100k_sequences_catch.fasta'))\n",
    "stupid_alignments = read_report(os.path.join(OUTPUTS_DIR, 'megares_100k_comparison.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mminbay\\AppData\\Local\\anaconda3\\envs\\bait_research\\lib\\site-packages\\pydivsufsort\\divsufsort.py:75: UserWarning: converting str argument uses more memory\n",
      "  warnings.warn(\"converting str argument uses more memory\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "non_exact = 0\n",
    "no_align = 0\n",
    "missed = 0\n",
    "for bait in baits:\n",
    "    bait = bait.upper()\n",
    "    search_res = sa_search(seq, sa, bait)\n",
    "    occurence = search_res[1]\n",
    "\n",
    "    in_sa = sa[occurence]\n",
    "    results = seed_and_extend(\n",
    "        seq,\n",
    "        bait,\n",
    "        40,\n",
    "        sa,\n",
    "        seed_len = 20,\n",
    "        seqlens = seqlens\n",
    "    )\n",
    "    if in_sa not in stupid_alignments.keys():\n",
    "        no_align += 1\n",
    "        continue\n",
    "    stupid = set(stupid_alignments[in_sa])\n",
    "    if stupid != results:\n",
    "        # print(in_sa)\n",
    "        # print(stupid)\n",
    "        # print(results)\n",
    "        non_exact += 1\n",
    "        missed += len(stupid.difference(results))\n",
    "print(non_exact)\n",
    "print(no_align)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810\n"
     ]
    }
   ],
   "source": [
    "print(missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated the following base repeats:\n",
      "['GCATCTCATCATTGTGGCCTTTCAAACCGGGCACGTACCCAAGTTTTGCTGACGTAATACTAGTGACCCCTACATTATCTATCATGGCTTGGCGGTCGAG', 'TCCGCGCGCAATACGCCTTAGTCCGTGTGGTAAGAGATTACATAGTTTCCGGGCCAGCAGACAAGTGATCGTATAGTATTAGCATATGGCGCGTGTCATA', 'TGCTCTTTGACTGCGCGCGATATAATCTGTTACTCGACCGCGCTTTTACGAAGTATGGAAGATGCAGGGCCTACATGCCACGTTTGAAAACGTACAATCA']\n",
      "Index 881 has been populated with the following:\n",
      "GCATCTCATCATTATGGCCTTTCAAACCGGGCACGTACCCAAGTTTTGCTGACGTAATACTAGTGACCCCTACATTATTTATCATGGCTTGGCGGGCGAG\n",
      "Index 152 has been populated with the following:\n",
      "GCATCTCATCACTGTGGCCATTCAAACCAGGCACGTACCCAAGTTTTGCTGACGTAATACTAGTGACCCCTACATTATCTATCATGGCTTGGCGGTCGAG\n",
      "Index 772 has been populated with the following:\n",
      "GCATCTCATTATTATGGCCTTTCAAACCGGGCACGTACCCAAGTTTTTCTGACGTAATACTAGTGACCCCTACATTATCTATCATGGCTTGGCGGTCGAG\n",
      "Index 12 has been populated with the following:\n",
      "GCATCTCATCATTGTGGCCTTTCAAACCGGGCACGTACCCAAGTTTTGCTGACGTAATACTAGAGACCCCTACATTATCTATCATGGCTTGGCGGTCGAA\n",
      "Index 693 has been populated with the following:\n",
      "TCCGAGCGTAATACGCCTTAGTCCGTGTGGTAAGAGATTACATAGTTTCCGGGCCAGCAGACAAGTGATCGTACAGTATTAGCATATGGCGCGTGGCATA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GTCACTTAAATAGCATCTCATCATTGTGGCCTTTCAAACCGGGCACGTACCCAAGTTTTGCTGACGTAATACTAGAGACCCCTACATTATCTATCATGGCTTGGCGGTCGAAGGAGCTCGTTTTACAGTAAGCAACGGACTTGTCCACGAGCGCATCTCATCACTGTGGCCATTCAAACCAGGCACGTACCCAAGTTTTGCTGACGTAATACTAGTGACCCCTACATTATCTATCATGGCTTGGCGGTCGAGGGCAGTACAGCTTCTCTATAGTAAACGACTACCCGCACTTTTTCAGGCCTTTCGGAATATATTGAGAGAGGATGGATAATTTACCCCTAATGGGATTTTGCCACCCGCGGGGTATCCCTTCGCAAGGAATACCGTGGCCGCCACCCATTACGTGGTAATCCGAACAGTGACGCCGACTTGATGATATGCGCGGGTACGGACCGGGGGTATGGTCCCGCACAGACACCAAAGATTCCTTCAATTGAGGGCTGTCCGGAGCGGAGTATCGCATGCGCACAGCCGCTGGGACTACTCAAACTGTACGGGCATATTGAAAGACCTCAATTACTCCCCTCAGGAAAATAGGGTAGGTGCGCTAAATTTATAAGAGTGACGCATTGGCCTCGAACCGTGTTGCCTAGTTTAATAGGCAAATGCGGATCGTCAGTTCCCAGCAAGGGTTCCGAGCGTAATACGCCTTAGTCCGTGTGGTAAGAGATTACATAGTTTCCGGGCCAGCAGACAAGTGATCGTACAGTATTAGCATATGGCGCGTGGCATATCAAACCGGGCACGTACCCAAGTTTTTCTGACGTAATACTAGTGACCCCTACATTATCTATCATGGCTTGGCGGTCGAGGATGAAAAAGCATCTCATCATTATGGCCTTTCAAACCGGGCACGTACCCAAGTTTTGCTGACGTAATACTAGTGACCCCTACATTATTTATCATGGCTTGGCGGGCGAGTGGTCATCGTGAGTTTCTC'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthesize_sequence(\n",
    "    1000,\n",
    "    3,\n",
    "    (100, 100),\n",
    "    0.5,\n",
    "    5,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
